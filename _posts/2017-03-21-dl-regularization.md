---
layout: post
title:  "深度学习正则化"
date:   2017-03-21
desc: "DL book chapter 7"
keywords: "reading, DL"
categories: [Deeplearning]
tags: [regularization]
icon: icon-html
---
## 以增大训练误差为代价来减少测试误差。（旨在提高泛化误差而不是训练误差）这些策略统称为正则化

1. 参数范数惩罚(L2 & L1)
2. 作为约束的范数惩罚
3. 正则化和欠约束问题
4. 数据集增强
5. **噪音鲁棒性**
6. 半监督学习
7. 多任务学习
8. early stopping
9. 参数绑定和参数共享 {偏好或者说是先验}
10. 稀疏表示{直接作用于激活单元，表示的正则化}
11. Bagging ensemble
12. 可以解释成廉价的n个模型的ensemble
13. 对抗训练
14. ...